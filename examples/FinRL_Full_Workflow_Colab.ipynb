{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_header"
   },
   "source": [
    "# FinRL Complete Workflow Example\n",
    "\n",
    "This notebook demonstrates the complete workflow from data acquisition to strategy training, stock selection, backtesting, and trade execution (Alpaca paper trading).\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Environment Setup** - Clone repository and install dependencies\n",
    "2. **Data Acquisition** - Fetch S&P 500 components and set research period\n",
    "3. **Fundamental Data** - Use `data_fetcher` to get real fundamental data with forward-looking return labels\n",
    "4. **ML Stock Selection** - Apply sector-rolling stock selection using `ml_strategy`\n",
    "5. **Backtesting** - Run backtest using the `bt` library and output performance metrics\n",
    "6. **Paper Trading** - Execute trades via Alpaca paper trading\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **FMP API Key**: Required for fundamental data (get free key at [Financial Modeling Prep](https://financialmodelingprep.com/))\n",
    "- **Alpaca API Keys**: Required for paper trading (sign up at [Alpaca](https://alpaca.markets/))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_1_setup"
   },
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "This section sets up the Google Colab environment by:\n",
    "- Cloning the FinRL-Trading repository\n",
    "- Installing required dependencies\n",
    "- Configuring API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": "# Clone the FinRL-Trading repository\n# This will download the complete codebase to your Colab environment\n\n!git clone -b claude/create-finrl-colab-notebook-nnPau https://github.com/vrangayyan6/FinRL-Trading.git\n\n# Change to the project directory\n%cd FinRL-Trading"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": "# Install all required dependencies\n# This may take a few minutes on first run\n\n# Install dependencies from requirements.txt\n!pip install -q -r requirements.txt\n\n# Add the project directory to Python path\n# This is required for importing src modules\nimport sys\nimport os\n\n# Get the current working directory (should be FinRL-Trading after %cd)\nproject_root = os.getcwd()\nif project_root not in sys.path:\n    sys.path.insert(0, project_root)\n\n# Also add src directory explicitly\nsrc_path = os.path.join(project_root, 'src')\nif src_path not in sys.path:\n    sys.path.insert(0, src_path)\n\nprint(f\"Project root: {project_root}\")\nprint(f\"Python path configured for imports.\")\n\n# Verify key packages are installed\nimport importlib\nrequired_packages = ['numpy', 'pandas', 'sklearn', 'lightgbm', 'yfinance', 'bt']\nmissing = []\nfor pkg in required_packages:\n    try:\n        importlib.import_module(pkg)\n    except ImportError:\n        missing.append(pkg)\n\nif missing:\n    print(f\"Warning: Some packages may need manual installation: {missing}\")\nelse:\n    print(\"All core dependencies installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_env"
   },
   "outputs": [],
   "source": [
    "# Configure API credentials\n",
    "# IMPORTANT: Replace the placeholder values with your actual API keys\n",
    "\n",
    "import os\n",
    "\n",
    "# Financial Modeling Prep (FMP) API Key\n",
    "# Get your free API key at: https://financialmodelingprep.com/developer/docs/\n",
    "os.environ['FMP_API_KEY'] = 'your_fmp_api_key_here'  # <-- REPLACE THIS\n",
    "\n",
    "# Alpaca Paper Trading API Keys (optional - only needed for trading section)\n",
    "# Sign up at: https://alpaca.markets/\n",
    "os.environ['ALPACA_API_KEY'] = 'your_alpaca_api_key_here'  # <-- REPLACE THIS\n",
    "os.environ['ALPACA_SECRET_KEY'] = 'your_alpaca_secret_key_here'  # <-- REPLACE THIS\n",
    "os.environ['ALPACA_BASE_URL'] = 'https://paper-api.alpaca.markets'  # Paper trading URL\n",
    "\n",
    "print(\"Environment variables configured.\")\n",
    "print(\"NOTE: Make sure to replace placeholder API keys with your actual keys!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alternative_env_setup"
   },
   "outputs": [],
   "source": [
    "# Alternative: Use Google Colab Secrets (Recommended for security)\n",
    "# Go to Colab menu: Runtime > Manage secrets\n",
    "# Add your API keys there, then uncomment and run this cell\n",
    "\n",
    "# from google.colab import userdata\n",
    "#\n",
    "# os.environ['FMP_API_KEY'] = userdata.get('FMP_API_KEY')\n",
    "# os.environ['ALPACA_API_KEY'] = userdata.get('ALPACA_API_KEY')\n",
    "# os.environ['ALPACA_SECRET_KEY'] = userdata.get('ALPACA_SECRET_KEY')\n",
    "# os.environ['ALPACA_BASE_URL'] = 'https://paper-api.alpaca.markets'\n",
    "#\n",
    "# print(\"API keys loaded from Colab secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_2_data"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Data Source Initialization and Stock Universe\n",
    "\n",
    "This section:\n",
    "- Initializes the data manager\n",
    "- Fetches S&P 500 component stocks\n",
    "- Defines the research time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_data_manager"
   },
   "outputs": [],
   "source": [
    "# Import data fetching utilities\n",
    "from src.data.data_fetcher import get_data_manager, fetch_sp500_tickers\n",
    "\n",
    "# Initialize the data manager\n",
    "# This will use FMP (Financial Modeling Prep) as the data source\n",
    "manager = get_data_manager()\n",
    "\n",
    "# Display data source information\n",
    "print(\"Data Manager Info:\")\n",
    "print(manager.get_source_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch_tickers"
   },
   "outputs": [],
   "source": [
    "# Fetch S&P 500 component stocks\n",
    "# If the API call fails, a fallback list will be used\n",
    "\n",
    "tickers = fetch_sp500_tickers()\n",
    "\n",
    "print(f\"Total S&P 500 components: {len(tickers)}\")\n",
    "print(\"\\nSample tickers (first 15):\")\n",
    "print(tickers.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_date_range"
   },
   "outputs": [],
   "source": [
    "# Define the research time period\n",
    "# A longer period provides more data for rolling model training\n",
    "\n",
    "start_date = '2015-10-15'  # Start date for historical data\n",
    "end_date = '2025-10-15'    # End date (can be adjusted to current date)\n",
    "\n",
    "print(f\"Research Period: {start_date} to {end_date}\")\n",
    "print(f\"Total years: ~{(2025 - 2015)} years of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_3_fundamentals"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Fetch Fundamental Data\n",
    "\n",
    "This section fetches fundamental financial data for all S&P 500 stocks.\n",
    "\n",
    "**Key Features:**\n",
    "- `align_quarter_dates=True`: Aligns trading dates to approximately 2 months after earnings release\n",
    "- Automatically calculates `y_return`: The next quarter's return (used as the prediction target)\n",
    "\n",
    "**Note:** First-time fetching may take several minutes. Data is cached locally for faster subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch_fundamentals"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.data.data_fetcher import fetch_fundamental_data\n",
    "\n",
    "# Fetch fundamental data for all tickers\n",
    "# Note: For faster demo, you can limit tickers: tickers_small = tickers[:80]\n",
    "\n",
    "print(\"Fetching fundamental data... This may take several minutes.\")\n",
    "\n",
    "fundamentals = fetch_fundamental_data(\n",
    "    tickers,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    align_quarter_dates=True,  # Align dates to avoid look-ahead bias\n",
    ")\n",
    "\n",
    "print(f\"\\nFundamental data shape: {fundamentals.shape}\")\n",
    "print(f\"Columns: {list(fundamentals.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_fundamentals"
   },
   "outputs": [],
   "source": [
    "# Validate the data contains required columns\n",
    "\n",
    "# Check for y_return (prediction target - next quarter's return)\n",
    "if 'y_return' not in fundamentals.columns:\n",
    "    raise ValueError(\"ERROR: 'y_return' column missing. Check data_fetcher implementation.\")\n",
    "\n",
    "print(\"Data validation passed!\")\n",
    "print(f\"\\nSample of y_return distribution:\")\n",
    "print(fundamentals['y_return'].describe())\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample data (first 5 rows):\")\n",
    "fundamentals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_fundamentals"
   },
   "outputs": [],
   "source": [
    "# Save the cleaned fundamental data for later use\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "clean_path = 'data/fundamentals.csv'\n",
    "fundamentals.to_csv(clean_path, index=False)\n",
    "\n",
    "print(f\"Saved cleaned fundamentals to: {clean_path}\")\n",
    "print(f\"File size: {os.path.getsize(clean_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_4_ml"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. ML Stock Selection Strategy\n",
    "\n",
    "This section uses machine learning to select stocks based on fundamental data.\n",
    "\n",
    "**Strategy Options:**\n",
    "- **Sector Neutral ML**: If sector data is available, performs sector-neutral stock selection\n",
    "- **Basic ML**: Falls back to basic ML selection if no sector data\n",
    "\n",
    "**Parameters:**\n",
    "- `test_quarters`: Number of quarters for testing\n",
    "- `top_quantile`: Select stocks in the top X% of predicted returns (0.9 = top 10%)\n",
    "- `prediction_mode`: 'rolling' for walk-forward, 'single' for one-time prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_strategy"
   },
   "outputs": [],
   "source": [
    "# Import ML strategy modules\n",
    "from src.strategies.base_strategy import StrategyConfig\n",
    "from src.strategies.ml_strategy import MLStockSelectionStrategy, SectorNeutralMLStrategy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the fundamental data\n",
    "fundamentals = pd.read_csv('data/fundamentals.csv')\n",
    "print(f\"Loaded fundamentals: {fundamentals.shape[0]} rows, {fundamentals.shape[1]} columns\")\n",
    "\n",
    "# Prepare data dictionary for the strategy\n",
    "data_dict = {'fundamentals': fundamentals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure_strategy"
   },
   "outputs": [],
   "source": [
    "# Configure both strategy options\n",
    "\n",
    "# Basic ML Stock Selection Strategy\n",
    "base_config = StrategyConfig(\n",
    "    name=\"ML Stock Selection\",\n",
    "    description=\"Machine learning based stock selection using fundamental data\"\n",
    ")\n",
    "base_strategy = MLStockSelectionStrategy(base_config)\n",
    "\n",
    "# Sector Neutral ML Strategy (used when sector data is available)\n",
    "sector_config = StrategyConfig(\n",
    "    name=\"Sector Neutral ML\",\n",
    "    description=\"Sector-neutral ML strategy for balanced portfolio construction\"\n",
    ")\n",
    "sector_strategy = SectorNeutralMLStrategy(sector_config)\n",
    "\n",
    "print(\"Strategies configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_strategy"
   },
   "outputs": [],
   "source": [
    "# Run the stock selection strategy\n",
    "\n",
    "# Strategy parameters\n",
    "test_quarters = 4      # Number of quarters to test\n",
    "top_quantile = 0.9     # Select top 10% of stocks (0.9 quantile)\n",
    "\n",
    "# Check if sector column exists\n",
    "sector_col_exists = ('sector' in fundamentals.columns) or ('gsector' in fundamentals.columns)\n",
    "\n",
    "if sector_col_exists:\n",
    "    # Use sector-neutral strategy for better diversification\n",
    "    print(\"Using Sector Neutral ML Strategy...\")\n",
    "    result = sector_strategy.generate_weights(\n",
    "        data_dict,\n",
    "        test_quarters=test_quarters,\n",
    "        top_quantile=top_quantile,\n",
    "        prediction_mode='rolling'\n",
    "    )\n",
    "else:\n",
    "    # Use basic ML strategy with single prediction mode\n",
    "    print(\"Using Basic ML Stock Selection Strategy...\")\n",
    "    result = base_strategy.generate_weights(\n",
    "        data=data_dict,\n",
    "        prediction_mode='single',  # Single prediction (not rolling)\n",
    "        test_quarters=test_quarters,\n",
    "        top_quantile=top_quantile,\n",
    "        weight_method='equal',     # Equal weight distribution\n",
    "        confirm_mode='today',\n",
    "        execution_date='2025-10-12'  # Order execution date\n",
    "    )\n",
    "\n",
    "# Extract weights from result\n",
    "weights = result.weights.copy()\n",
    "print(f\"\\nGenerated weights for {len(weights)} stock positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_weights"
   },
   "outputs": [],
   "source": [
    "# Save the generated weights to CSV\n",
    "\n",
    "out_dir = 'data'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "if sector_col_exists:\n",
    "    out_path = os.path.join(out_dir, 'ml_weights_sector.csv')\n",
    "else:\n",
    "    out_path = os.path.join(out_dir, 'ml_weights_today.csv')\n",
    "\n",
    "weights.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved portfolio weights to: {out_path}\")\n",
    "print(f\"\\nWeight distribution summary:\")\n",
    "print(weights.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_weights"
   },
   "outputs": [],
   "source": [
    "# View the selected stocks and their weights\n",
    "\n",
    "print(\"\\nTop selected stocks by weight:\")\n",
    "print(weights.nlargest(10, 'weight') if 'weight' in weights.columns else weights.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_5_backtest"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Backtest the Strategy\n",
    "\n",
    "This section backtests the ML-generated portfolio weights using the `bt` library.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the portfolio weights\n",
    "2. Fetch historical price data\n",
    "3. Run the backtest simulation\n",
    "4. Calculate and display performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_weights_backtest"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.backtest.backtest_engine import BacktestConfig, BacktestEngine\n",
    "from src.data.data_fetcher import fetch_price_data\n",
    "\n",
    "# Load the saved weights\n",
    "weights_path = out_path  # From previous section\n",
    "weights_raw = pd.read_csv(weights_path)\n",
    "\n",
    "# Validate required columns\n",
    "required_cols = {'date', 'gvkey', 'weight'}\n",
    "assert required_cols.issubset(weights_raw.columns), f'Weight file must contain: {required_cols}'\n",
    "\n",
    "# Parse and sort the data\n",
    "weights_raw['date'] = pd.to_datetime(weights_raw['date'])\n",
    "weights_raw['gvkey'] = weights_raw['gvkey'].astype(str)\n",
    "weights_raw = weights_raw.sort_values(['date', 'gvkey'])\n",
    "\n",
    "print(f\"Loaded {len(weights_raw)} weight entries\")\n",
    "print(f\"Date range: {weights_raw['date'].min()} to {weights_raw['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_weight_matrix"
   },
   "outputs": [],
   "source": [
    "# Create weight matrix (rows: dates, columns: tickers)\n",
    "\n",
    "weight_signals = (\n",
    "    weights_raw\n",
    "    .pivot_table(index='date', columns='gvkey', values='weight', aggfunc='sum')\n",
    "    .fillna(0.0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Remove rows where all weights are zero\n",
    "weight_signals = weight_signals.loc[(weight_signals.sum(axis=1) > 0.0)]\n",
    "\n",
    "if len(weight_signals) == 0:\n",
    "    raise ValueError('Weight matrix is empty - cannot run backtest')\n",
    "\n",
    "print(f\"Weight matrix shape: {weight_signals.shape}\")\n",
    "print(f\"Unique rebalance dates: {len(weight_signals)}\")\n",
    "print(f\"Unique tickers: {len(weight_signals.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configure_backtest"
   },
   "outputs": [],
   "source": [
    "# Configure the backtest\n",
    "\n",
    "# Determine backtest period from weights\n",
    "t_start = weight_signals.index.min().strftime('%Y-%m-%d')\n",
    "t_end = weight_signals.index.max().strftime('%Y-%m-%d')\n",
    "\n",
    "# Create backtest configuration\n",
    "backtest_config = BacktestConfig(\n",
    "    start_date=t_start,\n",
    "    end_date=t_end,\n",
    "    rebalance_freq='Q',        # Quarterly rebalancing\n",
    "    initial_capital=1000000.0  # $1 million initial capital\n",
    ")\n",
    "\n",
    "print(f\"Backtest Configuration:\")\n",
    "print(f\"  Period: {t_start} to {t_end}\")\n",
    "print(f\"  Rebalance Frequency: Quarterly\")\n",
    "print(f\"  Initial Capital: ${backtest_config.initial_capital:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetch_price_data"
   },
   "outputs": [],
   "source": [
    "# Fetch price data for backtest tickers\n",
    "\n",
    "tickers_bt = weight_signals.columns.tolist()\n",
    "print(f\"Fetching price data for {len(tickers_bt)} tickers...\")\n",
    "\n",
    "prices_long = fetch_price_data(\n",
    "    tickers_bt,\n",
    "    backtest_config.start_date,\n",
    "    backtest_config.end_date\n",
    ")\n",
    "\n",
    "if prices_long is not None and len(prices_long) > 0:\n",
    "    print(f\"Price data shape: {prices_long.shape}\")\n",
    "else:\n",
    "    print(\"Warning: No price data fetched. Backtest may not work.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_backtest"
   },
   "outputs": [],
   "source": [
    "# Run the backtest\n",
    "from pandas import DataFrame\n",
    "\n",
    "# Initialize backtest engine\n",
    "engine = BacktestEngine(backtest_config)\n",
    "\n",
    "# Prepare price data for bt library format\n",
    "price_data_bt = engine._prepare_price_data_for_bt(prices_long)\n",
    "\n",
    "# Align weights with available price data\n",
    "common_cols = [c for c in weight_signals.columns if c in price_data_bt.columns]\n",
    "weight_signals = weight_signals[common_cols]\n",
    "\n",
    "# Align weight dates to trading days\n",
    "trading_index = price_data_bt.index\n",
    "pos = trading_index.get_indexer(weight_signals.index, method='bfill')\n",
    "mask = pos != -1\n",
    "weight_signals = weight_signals.iloc[mask]\n",
    "weight_signals.index = trading_index[pos[mask]]\n",
    "\n",
    "# Clean and normalize weights for each date\n",
    "aligned = []\n",
    "for dt, row in weight_signals.iterrows():\n",
    "    prices_today = price_data_bt.loc[dt, row.index]\n",
    "    valid_cols = prices_today.dropna().index.tolist()\n",
    "    if len(valid_cols) == 0:\n",
    "        continue\n",
    "    row_valid = row[valid_cols]\n",
    "    s = row_valid.sum()\n",
    "    if s <= 0:\n",
    "        continue\n",
    "    aligned.append((dt, (row_valid / s)))\n",
    "\n",
    "if aligned:\n",
    "    weight_signals = DataFrame({dt: vec for dt, vec in aligned}).T.sort_index()\n",
    "else:\n",
    "    raise ValueError('No valid weight rows after alignment')\n",
    "\n",
    "# Final normalization (ensure weights sum to 1)\n",
    "weight_signals = weight_signals.div(weight_signals.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "print(f\"Final weight matrix: {weight_signals.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_results"
   },
   "outputs": [],
   "source": [
    "# Execute backtest and display results\n",
    "\n",
    "result = engine.run_backtest('ML Weights Strategy', prices_long, weight_signals)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BACKTEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Backtest Period: {t_start} to {t_end}\")\n",
    "print(f\"\\nPortfolio Annualized Return: {result.annualized_return:.2%}\")\n",
    "\n",
    "# Display benchmark comparisons\n",
    "if result.benchmark_annualized:\n",
    "    print(\"\\nBenchmark Comparisons:\")\n",
    "    for bm, ann in result.benchmark_annualized.items():\n",
    "        print(f\"  {bm} Annualized Return: {ann:.2%}\")\n",
    "\n",
    "# Display detailed metrics\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "metrics_df = result.to_metrics_dataframe()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_6_trading"
   },
   "source": [
    "---\n",
    "\n",
    "## 6. Alpaca Paper Trading\n",
    "\n",
    "This section demonstrates live paper trading execution using Alpaca.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Valid Alpaca API keys configured in Section 1\n",
    "- Paper trading account enabled\n",
    "\n",
    "**Note:** This section uses paper trading (simulated) and will NOT execute real trades or use real money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_executor"
   },
   "outputs": [],
   "source": [
    "# Initialize the trade executor\n",
    "import os\n",
    "from src.trading.trade_executor import create_trade_executor_from_env\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # Create trade executor from environment variables\n",
    "    executor = create_trade_executor_from_env()\n",
    "    print(\"Trade executor initialized successfully!\")\n",
    "    print(f\"Available accounts: {executor.alpaca.get_available_accounts()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Alpaca initialization failed: {e}\")\n",
    "    print(\"\\nTo enable this section:\")\n",
    "    print(\"1. Sign up at https://alpaca.markets/\")\n",
    "    print(\"2. Get your API keys from the dashboard\")\n",
    "    print(\"3. Configure them in Section 1 of this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_target_weights"
   },
   "outputs": [],
   "source": [
    "# Prepare target weights for trading\n",
    "\n",
    "try:\n",
    "    # Load the weights file\n",
    "    weights_path = 'data/ml_weights_today.csv'\n",
    "    df = pd.read_csv(weights_path)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "    # Get the latest date's weights\n",
    "    latest_date = df['date'].max()\n",
    "    df_latest = df[df['date'] == latest_date].copy()\n",
    "\n",
    "    print(f\"Using weights from: {latest_date}\")\n",
    "    print(f\"Number of positions: {len(df_latest)}\")\n",
    "\n",
    "    # Normalize weights to sum to 1\n",
    "    df_latest = df_latest.groupby('gvkey', as_index=False)['weight'].sum()\n",
    "    df_latest = df_latest[df_latest['weight'] > 0]\n",
    "    total_weight = df_latest['weight'].sum()\n",
    "\n",
    "    if total_weight <= 0:\n",
    "        raise ValueError('Total weight is zero - cannot create portfolio')\n",
    "\n",
    "    df_latest['weight'] = df_latest['weight'] / total_weight\n",
    "\n",
    "    # Convert to dictionary format for executor\n",
    "    target_weights = {row['gvkey']: float(row['weight']) for _, row in df_latest.iterrows()}\n",
    "\n",
    "    print(f\"\\nTarget portfolio ({len(target_weights)} positions):\")\n",
    "    for ticker, weight in sorted(target_weights.items(), key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {ticker}: {weight:.2%}\")\n",
    "    if len(target_weights) > 10:\n",
    "        print(f\"  ... and {len(target_weights) - 10} more positions\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing weights: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dry_run"
   },
   "outputs": [],
   "source": [
    "# Generate dry-run plan (preview without execution)\n",
    "\n",
    "try:\n",
    "    # Execute rebalance with dry_run=True (no actual orders)\n",
    "    plan = executor.alpaca.execute_portfolio_rebalance(\n",
    "        target_weights,\n",
    "        account_name='default',\n",
    "        dry_run=True  # Preview only\n",
    "    )\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"DRY-RUN TRADING PLAN\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Market Open: {plan.get('market_open')}\")\n",
    "    print(f\"Time in Force: {plan.get('used_time_in_force')}\")\n",
    "    print(f\"\\nPlanned Sell Orders: {len(plan.get('orders_plan', {}).get('sell', []))}\")\n",
    "    print(f\"Planned Buy Orders:  {len(plan.get('orders_plan', {}).get('buy', []))}\")\n",
    "\n",
    "    # Show sample buy orders\n",
    "    buy_orders = plan.get('orders_plan', {}).get('buy', [])[:5]\n",
    "    if buy_orders:\n",
    "        print(\"\\nSample Buy Orders:\")\n",
    "        for order in buy_orders:\n",
    "            print(f\"  {order['symbol']}: {order['quantity']:.2f} shares ({order['order_type']})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Dry-run failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "execute_trades"
   },
   "outputs": [],
   "source": [
    "# Execute paper trading (OPTIONAL - uncomment to run)\n",
    "# WARNING: This will submit actual paper trading orders!\n",
    "\n",
    "# Set this to True to enable order submission\n",
    "ENABLE_TRADING = False\n",
    "\n",
    "if ENABLE_TRADING:\n",
    "    try:\n",
    "        if plan.get('market_open'):\n",
    "            # Market is open - submit regular orders\n",
    "            submit = executor.alpaca.execute_portfolio_rebalance(\n",
    "                target_weights,\n",
    "                account_name='default'\n",
    "            )\n",
    "            print(f\"Orders submitted during market hours.\")\n",
    "            print(f\"Orders placed: {submit.get('orders_placed', 0)}\")\n",
    "        else:\n",
    "            # Market is closed - use OPG (On Open) orders\n",
    "            submit = executor.alpaca.execute_portfolio_rebalance(\n",
    "                target_weights,\n",
    "                account_name='default',\n",
    "                market_closed_action='opg'  # Execute at market open\n",
    "            )\n",
    "            print(f\"OPG orders submitted (will execute at market open).\")\n",
    "            print(f\"Orders placed: {submit.get('orders_placed', 0)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trade execution failed: {e}\")\n",
    "else:\n",
    "    print(\"Trading is disabled. Set ENABLE_TRADING = True to submit orders.\")\n",
    "    print(\"Note: This uses paper trading - no real money involved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_7_analysis"
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Performance Analysis\n",
    "\n",
    "This section analyzes the paper trading account performance and compares it to benchmarks (SPY, QQQ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analyze_performance"
   },
   "outputs": [],
   "source": [
    "# Analyze Alpaca account performance\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from src.trading.performance_analyzer import (\n",
    "    create_alpaca_account_from_env, AlpacaManager,\n",
    "    get_first_order_date, get_portfolio_history,\n",
    "    get_benchmark_data, display_metrics_table, plot_performance\n",
    ")\n",
    "\n",
    "try:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Create Alpaca manager\n",
    "    account = create_alpaca_account_from_env()\n",
    "    manager = AlpacaManager([account])\n",
    "\n",
    "    # Determine analysis period\n",
    "    end_date = datetime.now(timezone.utc)\n",
    "    first_order_date = get_first_order_date(manager)\n",
    "\n",
    "    if first_order_date is None:\n",
    "        raise RuntimeError('No order history found. Please execute some trades first.')\n",
    "\n",
    "    # Set date range (from first order to now)\n",
    "    start_date = first_order_date - timedelta(days=1)\n",
    "    start_date_str = start_date.date().isoformat()\n",
    "    end_date_str = (end_date + timedelta(days=1)).date().isoformat()\n",
    "\n",
    "    print(f\"Analysis Period: {start_date_str} to {end_date_str}\")\n",
    "\n",
    "    # Fetch portfolio and benchmark data\n",
    "    portfolio_df = get_portfolio_history(manager, start_date, end_date)\n",
    "    benchmark_df = get_benchmark_data(start_date_str, end_date_str)\n",
    "\n",
    "    # Display metrics comparison\n",
    "    display_metrics_table(portfolio_df, benchmark_df)\n",
    "\n",
    "    # Plot performance chart\n",
    "    plot_performance(portfolio_df, benchmark_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Performance analysis failed: {e}\")\n",
    "    print(\"\\nTo enable this section:\")\n",
    "    print(\"1. Configure Alpaca API keys\")\n",
    "    print(\"2. Execute some paper trades\")\n",
    "    print(\"3. Wait for order history to be available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_8_cleanup"
   },
   "source": [
    "---\n",
    "\n",
    "## 8. Cancel Open Orders (Optional)\n",
    "\n",
    "Use this section to cancel all pending/open orders if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cancel_orders"
   },
   "outputs": [],
   "source": [
    "# Cancel all open orders (OPTIONAL)\n",
    "from src.trading.performance_analyzer import create_alpaca_account_from_env, AlpacaManager\n",
    "\n",
    "# Set this to True to enable order cancellation\n",
    "ENABLE_CANCEL = False\n",
    "\n",
    "if ENABLE_CANCEL:\n",
    "    try:\n",
    "        account = create_alpaca_account_from_env()\n",
    "        manager = AlpacaManager([account])\n",
    "\n",
    "        # Check current open orders\n",
    "        open_orders = manager.get_orders(status='open', limit=200)\n",
    "        print(f\"Current open orders: {len(open_orders)}\")\n",
    "\n",
    "        if len(open_orders) > 0:\n",
    "            # Cancel all open orders\n",
    "            cancelled = manager.cancel_all_orders()\n",
    "            print(f\"Cancelled orders: {cancelled}\")\n",
    "        else:\n",
    "            print(\"No open orders to cancel.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Cancel orders failed: {e}\")\n",
    "else:\n",
    "    print(\"Order cancellation is disabled. Set ENABLE_CANCEL = True to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete FinRL workflow:\n",
    "\n",
    "1. **Environment Setup**: Configured Colab with required dependencies and API keys\n",
    "2. **Data Acquisition**: Fetched S&P 500 tickers and fundamental data\n",
    "3. **ML Stock Selection**: Applied machine learning to select top-performing stocks\n",
    "4. **Backtesting**: Evaluated strategy performance against historical data\n",
    "5. **Paper Trading**: Executed (simulated) trades via Alpaca\n",
    "6. **Performance Analysis**: Compared portfolio returns to benchmarks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different `top_quantile` values (e.g., 0.8 for top 20%)\n",
    "- Try rolling prediction mode for more robust results\n",
    "- Add sector data for sector-neutral strategy\n",
    "- Adjust rebalancing frequency in backtest config\n",
    "- Monitor paper trading performance over time\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [FinRL-Trading GitHub](https://github.com/AI4Finance-Foundation/FinRL-Trading)\n",
    "- [Financial Modeling Prep API](https://financialmodelingprep.com/)\n",
    "- [Alpaca Markets](https://alpaca.markets/)\n",
    "\n",
    "---\n",
    "\n",
    "*Created with FinRL-Trading - AI-Powered Stock Selection and Trading*"
   ]
  }
 ]
}