{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinRL Full Workflow — Daily Rebalancing\n",
    "\n",
    "This notebook demonstrates a **daily ML stock selection pipeline** — combining quarterly fundamental ratios with daily technical indicators to predict next-day returns, then rebalancing a portfolio every trading day.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Fetch Fundamentals** — Download quarterly financial ratios (EPS, P/E, ROE, debt ratios, etc.) for all S&P 500 stocks from Yahoo Finance. These are updated once per quarter when new earnings data becomes available.\n",
    "\n",
    "2. **Fetch Daily Prices & Compute Features** — Download daily price data and compute technical indicators: momentum (5/10/20-day), volatility (20/60-day), moving averages (SMA 5/10/20/50/200), RSI (14-day), and MACD. Each daily row is merged with the most recent quarterly fundamentals via `merge_asof`, giving the model ~26 features per stock per day.\n",
    "\n",
    "3. **ML Stock Selection (Daily)** — Train a LightGBM model on the combined daily+fundamental features to predict next-day forward log returns. The model trains on ~504 trading days (2 years) of history and validates on ~63 days (1 quarter). Select the top 10% of stocks (`top_quantile=0.9`) and assign equal weights.\n",
    "\n",
    "4. **Backtest** — Run a daily-rebalanced backtest using the `bt` library, comparing the ML portfolio against SPY and QQQ benchmarks.\n",
    "\n",
    "5. **Paper Trading** — Submit the daily portfolio weights as orders to Alpaca paper trading.\n",
    "\n",
    "## How Often to Run Each Section\n",
    "\n",
    "This is a **daily rebalancing strategy**. The recommended cadence:\n",
    "\n",
    "| Section | How Often | When |\n",
    "|---------|-----------|------|\n",
    "| **Fetch Fundamentals** (Cell 6) | Once per quarter | ~2 months after quarter ends (end of Feb/May/Aug/Nov) |\n",
    "| **Fetch Daily Prices & Features** (Cell 8) | Daily | Each trading day before ML selection |\n",
    "| **ML Stock Selection** (Cell 10) | Daily | After computing daily features |\n",
    "| **Backtest** (Cell 12) | After each ML run | Sanity check on cumulative performance |\n",
    "| **Paper Trading** (Cells 14-16) | Daily | Rebalance portfolio each trading day |\n",
    "| **Performance Analysis** (Cell 17) | Anytime | Check portfolio performance vs benchmarks |\n",
    "| **Cancel Orders** (Cell 19) | As needed | Only to revoke pending orders |\n",
    "\n",
    "### Typical Daily Workflow\n",
    "\n",
    "```\n",
    "Each trading day (before market open):\n",
    "  1. Run Cells 1-8   (fetch fresh daily prices + compute features)\n",
    "  2. Run Cell 10     (generate today's stock picks via daily ML)\n",
    "  3. Run Cell 12     (backtest sanity check — optional, can skip on busy days)\n",
    "  4. Run Cells 14-16 (submit rebalance orders to Alpaca)\n",
    "\n",
    "Once per quarter (late Feb / May / Aug / Nov):\n",
    "  1. Run Cells 1-6   (refresh quarterly fundamentals from Yahoo Finance)\n",
    "  2. Then continue with the daily cycle above\n",
    "```\n",
    "\n",
    "## Data Sources & Features\n",
    "\n",
    "- **Yahoo Finance** for both fundamental and price data — no API key required.\n",
    "- **Quarterly features** (14): EPS, BPS, DPS, current/quick/cash ratios, debt ratio, debt-to-equity, P/E, P/S, P/B, ROE, net income ratio.\n",
    "- **Daily technical features** (~12): momentum (5d/10d/20d), volatility (20d/60d), SMA (5/10/20/50/200), RSI-14, MACD, MACD signal.\n",
    "- **Target variable**: next-day forward log return.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Install dependencies: run `pip install -r requirements.txt` from the project root\n",
    "- Alpaca API keys are only needed for paper trading (Cells 14+). Cells 1-12 work without any API keys.\n",
    "- To use Alpaca, configure `.env` (see `README.md` / `.env.example`)\n",
    "- The first run will generate cache files in `data/cache/` to avoid redundant data fetching\n",
    "- Research window is ~4 years (need 200+ days for SMA-200 warmup + 568 days for training/validation/prediction)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrang\\Downloads\\FinRL-Trading\\.claude\\worktrees\\xenodochial-khayyam\n"
     ]
    }
   ],
   "source": [
    "%cd \"~/Downloads/FinRL-Trading\\.claude\\worktrees\\xenodochial-khayyam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Environment and path setup\n",
    "# import os, sys, logging\n",
    "# from datetime import datetime, timedelta\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Infer project root from working directory (this file is in examples/)\n",
    "# nb_dir = Path().resolve()\n",
    "# project_root = str(nb_dir.parent)\n",
    "# print(f\"project_root: {project_root}\")\n",
    "# if project_root not in sys.path:\n",
    "#     sys.path.insert(0, project_root)\n",
    "#     sys.path.insert(0, os.path.join(project_root, 'src'))\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: C:\\Users\\vrang\\Downloads\\FinRL-Trading\\.claude\\worktrees\\xenodochial-khayyam\n",
      "project_root: C:\\Users\\vrang\\Downloads\\FinRL-Trading\\.claude\\worktrees\\xenodochial-khayyam\n",
      "{'current_source': 'YAHOO', 'available_sources': ['FMP', 'YAHOO'], 'cache_dir': './data/cache'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vrang\\Downloads\\FinRL-Trading\\.claude\\worktrees\\xenodochial-khayyam\\src\\data\\data_fetcher.py:1062: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tickers: 503 (show first 15)\n",
      "   tickers                 sectors dateFirstAdded\n",
      "0      MMM             Industrials     1957-03-04\n",
      "1      AOS             Industrials     2017-07-26\n",
      "2      ABT             Health Care     1957-03-04\n",
      "3     ABBV             Health Care     2012-12-31\n",
      "4      ACN  Information Technology     2011-07-06\n",
      "5     ADBE  Information Technology     1997-05-05\n",
      "6      AMD  Information Technology     2017-03-20\n",
      "7      AES               Utilities     1998-10-02\n",
      "8      AFL              Financials     1999-05-28\n",
      "9        A             Health Care     2000-06-05\n",
      "10     APD               Materials     1985-04-30\n",
      "11    ABNB  Consumer Discretionary     2023-09-18\n",
      "12    AKAM  Information Technology     2007-07-12\n",
      "13     ALB               Materials     2016-07-01\n",
      "14     ARE             Real Estate     2017-03-20\n",
      "Research period: 2022-02-16 -> 2026-02-15\n"
     ]
    }
   ],
   "source": [
    "# Data source initialization and stock universe / date range setup\n",
    "from src.data.data_fetcher import get_data_manager, fetch_sp500_tickers\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize the data manager with Yahoo Finance (no API key required)\n",
    "manager = get_data_manager(preferred_source='YAHOO')\n",
    "print(manager.get_source_info())\n",
    "\n",
    "# Fetch S&P 500 components (falls back to a default list on failure)\n",
    "tickers = fetch_sp500_tickers(preferred_source='YAHOO')\n",
    "print(f\"Total tickers: {len(tickers)} (show first 15)\")\n",
    "print(tickers.head(15))\n",
    "\n",
    "# Research time period (~4 years — need 200+ days for SMA-200 warmup + 568 days for train/val/predict)\n",
    "end_date_dt = datetime.now()\n",
    "start_date_dt = end_date_dt - timedelta(days=int(4 * 365))\n",
    "\n",
    "start_date = start_date_dt.strftime('%Y-%m-%d')\n",
    "end_date = end_date_dt.strftime('%Y-%m-%d')\n",
    "print(f\"Research period: {start_date} -> {end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 — Fetch Fundamentals\n",
    "\n",
    "Download quarterly financial ratios (EPS, P/E, ROE, debt ratios, etc.) for S&P 500 stocks from Yahoo Finance.\n",
    "The fetcher computes `y_return` (log return to the next quarter) and optionally aligns dates to avoid look-ahead bias.\n",
    "\n",
    "Saved to `data/fundamentals.csv` for reuse in subsequent stages.\n",
    "\n",
    "**When to run:** Once per quarter, ~2 months after the quarter ends (late Feb / May / Aug / Nov), when new earnings data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Yahoo Finance data: 100%|███████████████████████████████████████████████████| 503/503 [07:46<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fundamentals shape: (2794, 20)\n",
      "Dropped 'gsector' column from fundamentals\n",
      "Saved cleaned fundamentals to data/fundamentals.csv\n",
      "Columns: ['gvkey', 'datadate', 'tic', 'adj_close_q', 'EPS', 'BPS', 'DPS', 'cur_ratio', 'quick_ratio', 'cash_ratio', 'acc_rec_turnover', 'debt_ratio', 'debt_to_equity', 'pe', 'ps', 'pb', 'roe', 'net_income_ratio', 'y_return']\n"
     ]
    }
   ],
   "source": [
    "# Fetch fundamental data — with CSV cache check\n",
    "import pandas as pd\n",
    "import os\n",
    "from src.data.data_fetcher import fetch_fundamental_data\n",
    "\n",
    "clean_path = 'data/fundamentals.csv'\n",
    "need_fetch = True\n",
    "\n",
    "# Check if cached CSV already covers the needed tickers and dates\n",
    "if os.path.exists(clean_path):\n",
    "    existing = pd.read_csv(clean_path)\n",
    "    existing['datadate'] = pd.to_datetime(existing['datadate'])\n",
    "\n",
    "    tickers_list = tickers['tickers'].tolist() if isinstance(tickers, pd.DataFrame) else list(tickers)\n",
    "    tickers_in_csv = set(existing['tic'].unique())\n",
    "    tickers_covered = sum(1 for t in tickers_list if t in tickers_in_csv)\n",
    "    coverage_pct = tickers_covered / len(tickers_list) * 100\n",
    "\n",
    "    date_min = existing['datadate'].min()\n",
    "    date_max = existing['datadate'].max()\n",
    "\n",
    "    # Accept if >=95% of tickers present and y_return exists\n",
    "    if coverage_pct >= 95 and 'y_return' in existing.columns:\n",
    "        print(f\"Found existing {clean_path}: {len(existing)} rows, {len(tickers_in_csv)} tickers\")\n",
    "        print(f\"  Ticker coverage: {tickers_covered}/{len(tickers_list)} ({coverage_pct:.0f}%)\")\n",
    "        print(f\"  Date range: {date_min.date()} to {date_max.date()}\")\n",
    "        print(f\"  Skipping Yahoo Finance fetch (takes ~7 min). Delete {clean_path} to force refresh.\")\n",
    "        fundamentals = existing\n",
    "        need_fetch = False\n",
    "    else:\n",
    "        print(f\"Found {clean_path} but coverage insufficient ({coverage_pct:.0f}% tickers). Re-fetching...\")\n",
    "\n",
    "if need_fetch:\n",
    "    fundamentals = fetch_fundamental_data(\n",
    "        tickers,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        align_quarter_dates=True,    # Shift trade dates ~2 months after earnings release to align across companies\n",
    "        preferred_source='YAHOO'     # Use Yahoo Finance (no API key needed)\n",
    "    )\n",
    "    print(f\"fundamentals shape: {fundamentals.shape}\")\n",
    "\n",
    "    # Basic validation: must contain y_return (next-quarter return computed by fetcher)\n",
    "    if 'y_return' not in fundamentals.columns:\n",
    "        raise ValueError(\"fundamentals missing y_return — check data_fetcher implementation\")\n",
    "\n",
    "    # Drop sector/gsector columns — not used for basic ML stock selection\n",
    "    for col in ['sector', 'gsector']:\n",
    "        if col in fundamentals.columns:\n",
    "            fundamentals = fundamentals.drop(columns=[col])\n",
    "            print(f\"Dropped '{col}' column from fundamentals\")\n",
    "\n",
    "    # Save a clean copy of the data\n",
    "    fundamentals.to_csv(clean_path, index=False)\n",
    "    print(f\"Saved cleaned fundamentals to {clean_path}\")\n",
    "\n",
    "print(f\"Columns: {list(fundamentals.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1b — Fetch Daily Prices & Compute Features\n",
    "\n",
    "Fetch daily price data for all S&P 500 tickers and compute technical indicators:\n",
    "momentum (5/10/20-day), volatility (20/60-day), moving averages (SMA 5–200), RSI-14, and MACD.\n",
    "\n",
    "Each daily row is then merged with the most recent quarterly fundamentals using `merge_asof`,\n",
    "so every row has both daily technicals and quarterly ratios as features.\n",
    "\n",
    "The forward 1-day log return (`y_return`) is computed as the ML prediction target.\n",
    "\n",
    "**When to run:** Every trading day, before running ML stock selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Yahoo price data: 100%|█████████████████████████████████████████████████████| 503/503 [01:08<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily prices: 500350 rows, 503 tickers\n",
      "Daily features after warmup removal: (400376, 24)\n",
      "Merged daily+fundamental data: (400376, 38)\n",
      "Feature columns: ['daily_return', 'momentum_5d', 'momentum_10d', 'momentum_20d', 'volatility_20d', 'volatility_60d', 'sma_5', 'sma_10', 'sma_20', 'sma_50', 'sma_200', 'rsi_14', 'macd', 'macd_signal', 'EPS', 'BPS', 'DPS', 'cur_ratio', 'quick_ratio', 'cash_ratio', 'acc_rec_turnover', 'debt_ratio', 'debt_to_equity', 'pe', 'ps', 'pb', 'roe', 'net_income_ratio']\n",
      "Saved to data/daily_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Fetch daily prices and compute technical indicators + merge with fundamentals\n",
    "\n",
    "# Re-derive tickers/dates if not already in kernel (e.g. after kernel restart)\n",
    "try:\n",
    "    tickers\n",
    "except NameError:\n",
    "    from src.data.data_fetcher import fetch_sp500_tickers\n",
    "    from datetime import datetime, timedelta\n",
    "    tickers = fetch_sp500_tickers(preferred_source='YAHOO')\n",
    "    end_date_dt = datetime.now()\n",
    "    start_date_dt = end_date_dt - timedelta(days=int(4 * 365))\n",
    "    start_date = start_date_dt.strftime('%Y-%m-%d')\n",
    "    end_date = end_date_dt.strftime('%Y-%m-%d')\n",
    "    print(f\"Re-derived: {len(tickers)} tickers, {start_date} -> {end_date}\")\n",
    "\n",
    "import pandas as pd\n",
    "from src.data.data_fetcher import fetch_price_data\n",
    "from src.data.data_processor import compute_daily_features, merge_daily_with_fundamentals\n",
    "\n",
    "# Fetch daily price data for all S&P 500 tickers\n",
    "prices = fetch_price_data(tickers, start_date, end_date, preferred_source='YAHOO')\n",
    "print(f\"Daily prices: {prices.shape[0]} rows, {prices['tic'].nunique()} tickers\")\n",
    "\n",
    "# Compute daily technical indicators and forward 1-day return (y_return)\n",
    "daily_features = compute_daily_features(prices)\n",
    "print(f\"Daily features after warmup removal: {daily_features.shape}\")\n",
    "\n",
    "# Merge with quarterly fundamentals (forward-fill latest quarter's ratios to each daily row)\n",
    "fundamentals = pd.read_csv('data/fundamentals.csv')\n",
    "daily_data = merge_daily_with_fundamentals(daily_features, fundamentals)\n",
    "print(f\"Merged daily+fundamental data: {daily_data.shape}\")\n",
    "print(f\"Feature columns: {[c for c in daily_data.columns if c not in ['gvkey','tic','datadate','y_return','adj_close','prccd','prcod','prchd','prcld','cshtrd']]}\")\n",
    "\n",
    "# Save for reuse\n",
    "daily_data.to_csv('data/daily_features.csv', index=False)\n",
    "print(f\"Saved to data/daily_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 — Daily ML Stock Selection\n",
    "\n",
    "Train a LightGBM model on ~26 features (14 quarterly fundamentals + ~12 daily technicals) to predict next-day forward log returns. Stocks in the top 10% (`top_quantile=0.9`) receive equal weights. Results are saved to `data/ml_weights_today.csv`.\n",
    "\n",
    "- **`frequency='daily'`** — Uses the daily training path with combined features.\n",
    "- **`train_days=504`** — Training window of ~2 years of trading days.\n",
    "- **`val_days=63`** — Validation window of ~1 quarter (63 trading days).\n",
    "- **Target**: `y_return` = forward 1-day log return.\n",
    "\n",
    "**When to run:** Every trading day, after computing daily features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_root: C:\\Users\\vrang\\Downloads\\FinRL-Trading\\.claude\\worktrees\\xenodochial-khayyam\n",
      "Loaded daily features: 400376 rows, 38 columns\n"
     ]
    }
   ],
   "source": [
    "# Daily ML stock selection and save weights CSV\n",
    "from src.strategies.base_strategy import StrategyConfig\n",
    "from src.strategies.ml_strategy import MLStockSelectionStrategy\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load daily features (daily technicals merged with quarterly fundamentals)\n",
    "daily_data = pd.read_csv('data/daily_features.csv')\n",
    "print(f\"Loaded daily features: {daily_data.shape[0]} rows, {daily_data.shape[1]} columns\")\n",
    "\n",
    "# Load fundamentals for reference (used by weight allocation if min_variance)\n",
    "fundamentals = pd.read_csv('data/fundamentals.csv')\n",
    "\n",
    "config = StrategyConfig(\n",
    "    name=\"Daily ML Stock Selection\",\n",
    "    description=\"Daily ML stock selection using technical + fundamental features\"\n",
    ")\n",
    "strategy = MLStockSelectionStrategy(config)\n",
    "\n",
    "data_dict = {\n",
    "    'fundamentals': fundamentals,\n",
    "    'daily': daily_data,\n",
    "}\n",
    "\n",
    "# Strategy parameters — daily mode\n",
    "train_days = 504       # ~2 years of trading days\n",
    "val_days = 63          # ~1 quarter for validation\n",
    "top_quantile = 0.9     # Top 10% of stocks by predicted return\n",
    "\n",
    "# Run daily ML stock selection\n",
    "res = strategy.generate_weights(\n",
    "    data=data_dict,\n",
    "    frequency='daily',\n",
    "    train_days=train_days,\n",
    "    val_days=val_days,\n",
    "    top_quantile=top_quantile,\n",
    "    weight_method='equal',\n",
    ")\n",
    "\n",
    "weights = res.weights.copy()\n",
    "print(f\"weights rows: {len(weights)}\")\n",
    "print(f\"Metadata: model={res.metadata.get('model_name')}, MSE={res.metadata.get('model_mse', 'N/A')}\")\n",
    "\n",
    "# Save weights\n",
    "out_dir = 'data'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, 'ml_weights_today.csv')\n",
    "weights.to_csv(out_path, index=False)\n",
    "print(f\"Saved weights to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 — Daily Backtest\n",
    "\n",
    "Run a daily-rebalanced backtest of the ML-selected portfolio weights using the **bt** library.\n",
    "The portfolio rebalances on each weight date, and weights are forward-filled to all trading days.\n",
    "Compares annualized return, Sharpe ratio, and drawdown against SPY and QQQ benchmarks.\n",
    "\n",
    "**When to run:** After each daily ML run — serves as a cumulative performance sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest weights using the bt library and output metrics\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from src.backtest.backtest_engine import BacktestConfig, BacktestEngine\n",
    "from src.data.data_fetcher import fetch_price_data\n",
    "\n",
    "# Load weights\n",
    "weights_path = 'data/ml_weights_today.csv'  # From previous cell\n",
    "weights_raw = pd.read_csv(weights_path)\n",
    "assert {'date','gvkey','weight'}.issubset(weights_raw.columns), 'Weight file must contain date/gvkey/weight'\n",
    "weights_raw['date'] = pd.to_datetime(weights_raw['date'])\n",
    "weights_raw['gvkey'] = weights_raw['gvkey'].astype(str)\n",
    "weights_raw = weights_raw.sort_values(['date','gvkey'])\n",
    "\n",
    "# Create weight matrix (rows: dates, columns: tickers)\n",
    "weight_signals = (\n",
    "    weights_raw\n",
    "    .pivot_table(index='date', columns='gvkey', values='weight', aggfunc='sum')\n",
    "    .fillna(0.0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Filter out rows where all weights are zero\n",
    "weight_signals = weight_signals.loc[(weight_signals.sum(axis=1) > 0.0)]\n",
    "assert len(weight_signals) > 0, 'Weight matrix is empty — cannot run backtest'\n",
    "\n",
    "# Backtest time period\n",
    "# Extend start date 90 days before earliest weight and end date to today\n",
    "# so bt has enough price history and can align weight dates to trading days\n",
    "t_min = weight_signals.index.min()\n",
    "t_max = weight_signals.index.max()\n",
    "t_start = (t_min - timedelta(days=90)).strftime('%Y-%m-%d')\n",
    "t_end = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Weight dates: {t_min.strftime('%Y-%m-%d')} to {t_max.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Price data range: {t_start} to {t_end}\")\n",
    "\n",
    "cfg = BacktestConfig(\n",
    "    start_date=t_start,\n",
    "    end_date=t_end,\n",
    "    rebalance_freq='D',    # Daily rebalancing\n",
    "    initial_capital=1000000.0\n",
    ")\n",
    "\n",
    "# Fetch price data from Yahoo Finance\n",
    "tickers_bt = weight_signals.columns.tolist()\n",
    "prices_long = fetch_price_data(tickers_bt, cfg.start_date, cfg.end_date, preferred_source='YAHOO')\n",
    "\n",
    "if prices_long is None or len(prices_long) == 0:\n",
    "    raise ValueError(f'No price data fetched for {len(tickers_bt)} tickers between {t_start} and {t_end}')\n",
    "print(f\"Fetched price data: {len(prices_long)} rows for {prices_long['gvkey'].nunique()} tickers\")\n",
    "\n",
    "# Run backtest\n",
    "engine = BacktestEngine(cfg)\n",
    "price_data_bt = engine._prepare_price_data_for_bt(prices_long)\n",
    "print(f\"Price matrix: {price_data_bt.shape[0]} trading days, {price_data_bt.shape[1]} tickers\")\n",
    "print(f\"Price date range: {price_data_bt.index.min().strftime('%Y-%m-%d')} to {price_data_bt.index.max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Align weight dates to nearest available trading day\n",
    "# Use 'nearest' so weights snap to the closest trading day in the price data\n",
    "from pandas import DataFrame\n",
    "common_cols = [c for c in weight_signals.columns if c in price_data_bt.columns]\n",
    "if len(common_cols) == 0:\n",
    "    raise ValueError(\n",
    "        f'No overlapping tickers between weights ({list(weight_signals.columns[:5])}) '\n",
    "        f'and prices ({list(price_data_bt.columns[:5])})'\n",
    "    )\n",
    "weight_signals = weight_signals[common_cols]\n",
    "trading_index = price_data_bt.index\n",
    "pos = trading_index.get_indexer(weight_signals.index, method='nearest')\n",
    "mask = pos != -1\n",
    "weight_signals = weight_signals.iloc[mask]\n",
    "weight_signals.index = trading_index[pos[mask]]\n",
    "\n",
    "# Remove stocks with no price on each date and normalize\n",
    "aligned = []\n",
    "for dt, row in weight_signals.iterrows():\n",
    "    prices_today = price_data_bt.loc[dt, row.index]\n",
    "    valid_cols = prices_today.dropna().index.tolist()\n",
    "    if len(valid_cols) == 0:\n",
    "        continue\n",
    "    row_valid = row[valid_cols]\n",
    "    s = row_valid.sum()\n",
    "    if s <= 0:\n",
    "        continue\n",
    "    aligned.append((dt, (row_valid / s)))\n",
    "\n",
    "if aligned:\n",
    "    weight_signals = DataFrame({dt: vec for dt, vec in aligned}).T.sort_index()\n",
    "else:\n",
    "    raise ValueError('No valid weight rows after alignment')\n",
    "\n",
    "# Normalize rows again\n",
    "weight_signals = weight_signals.div(weight_signals.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "result = engine.run_backtest('Daily ML Strategy', prices_long, weight_signals)\n",
    "print(f\"Backtest period: {t_start} to {t_end}\")\n",
    "print(f\"Portfolio annualized return: {result.annualized_return:.2%}\")\n",
    "for bm, ann in (result.benchmark_annualized or {}).items():\n",
    "    print(f\"Benchmark {bm} annualized return: {ann:.2%}\")\n",
    "\n",
    "metrics_df = result.to_metrics_dataframe()\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpaca Paper Trading\n",
    "\n",
    "This section connects to your **Alpaca paper trading** account to execute the daily ML-selected portfolio.\n",
    "\n",
    "**Cells in this section:**\n",
    "- **Cell 14** — Initializes the Alpaca API, loads the latest weights from `data/ml_weights_today.csv`, and generates a **dry-run** rebalance plan (no orders submitted). Review the plan before proceeding.\n",
    "- **Cell 15** — Inspect the planned buy orders in detail.\n",
    "- **Cell 16** — Actually **submit** the rebalance orders. If the market is closed and `use_opg=True`, orders are submitted as OPG (On-Open) to execute at the next market open.\n",
    "- **Cell 17** — Fetch your Alpaca account equity curve and compare performance against SPY/QQQ benchmarks.\n",
    "\n",
    "**Prerequisites:** Alpaca API keys must be configured in `.env` (see `README.md` / `.env.example`).\n",
    "\n",
    "**When to run:** Daily — each trading day after generating new ML weights to rebalance the portfolio. Cell 17 can be run anytime to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Alpaca account status, generate dry-run plan, and optionally execute paper trades\n",
    "import os\n",
    "from src.trading.trade_executor import create_trade_executor_from_env\n",
    "import pandas as pd\n",
    "\n",
    "# Only runs if Alpaca API is configured. Otherwise raises an error with instructions.\n",
    "try:\n",
    "    executor = create_trade_executor_from_env()\n",
    "    print(\"Trade executor initialized. Accounts:\", executor.alpaca.get_available_accounts())\n",
    "\n",
    "    weights_path = 'data/ml_weights_today.csv'  # From daily ML stock selection cell\n",
    "    # Use the weights from the previous cell, take the latest rebalance date\n",
    "    df = pd.read_csv(weights_path)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "    latest_date = df['date'].max()\n",
    "    df_latest = df[df['date'] == latest_date].copy()\n",
    "\n",
    "    # Normalize and convert to target weights\n",
    "    df_latest = df_latest.groupby('gvkey', as_index=False)['weight'].sum()\n",
    "    df_latest = df_latest[df_latest['weight'] > 0]\n",
    "    s = df_latest['weight'].sum()\n",
    "    if s <= 0:\n",
    "        raise ValueError('Total weight on latest rebalance date is zero')\n",
    "    df_latest['weight'] = df_latest['weight'] / s\n",
    "    target_weights = {row['gvkey']: float(row['weight']) for _, row in df_latest.iterrows()}\n",
    "\n",
    "    # Generate dry-run plan (no order submission)\n",
    "    plan = executor.alpaca.execute_portfolio_rebalance(\n",
    "        target_weights,\n",
    "        account_name='default',\n",
    "        dry_run=True\n",
    "    )\n",
    "    print('Dry-run plan generated:')\n",
    "    print('Market open:', plan.get('market_open'))\n",
    "    print('Used TIF:', plan.get('used_time_in_force'))\n",
    "    print('Planned sells:', len(plan.get('orders_plan', {}).get('sell', [])))\n",
    "    print('Planned buys :', len(plan.get('orders_plan', {}).get('buy',  [])))\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print('Alpaca initialization or execution failed:', e)\n",
    "    print('To enable this cell, configure .env and refer to the Alpaca setup instructions in README.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.get('orders_plan', {}).get('buy',  [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit orders based on market status (paper trading)\n",
    "# use_opg = os.getenv('USE_OPG', 'false').lower() == 'true'\n",
    "use_opg = True\n",
    "if plan.get('market_open'):\n",
    "    submit = executor.alpaca.execute_portfolio_rebalance(\n",
    "        target_weights,\n",
    "        account_name='default'\n",
    "    )\n",
    "    print('Submitted during market hours. Orders placed:', submit.get('orders_placed', 0))\n",
    "else:\n",
    "    if use_opg:\n",
    "        submit = executor.alpaca.execute_portfolio_rebalance(\n",
    "            target_weights,\n",
    "            account_name='default',\n",
    "            market_closed_action='opg'\n",
    "        )\n",
    "        print('Submitted as OPG. Orders placed:', submit.get('orders_placed', 0))\n",
    "    else:\n",
    "        print('Market is closed. Skipping submission. Set USE_OPG=true to submit OPG at open.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Alpaca account history and compare against benchmarks (SPY/QQQ)\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from src.trading.performance_analyzer import (\n",
    "    create_alpaca_account_from_env, AlpacaManager,\n",
    "    get_first_order_date, get_portfolio_history,\n",
    "    get_benchmark_data, display_metrics_table, plot_performance\n",
    ")\n",
    "\n",
    "try:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    # Create AlpacaManager\n",
    "    account = create_alpaca_account_from_env()\n",
    "    manager = AlpacaManager([account])\n",
    "\n",
    "    # Time range: from first order date to now (you can also customize the window)\n",
    "    end_date = datetime.now(timezone.utc)\n",
    "    first_order_date = get_first_order_date(manager)\n",
    "    if first_order_date is None:\n",
    "        raise RuntimeError('Cannot determine first order date — ensure account has trade history or check API permissions')\n",
    "\n",
    "    # Ensure we fetch data through end_date (FMP requires +1 day), and start_date-1 for boundary\n",
    "    start_date = first_order_date - timedelta(days=1)\n",
    "    start_date_str = start_date.date().isoformat()\n",
    "    fmp_end_date = end_date + timedelta(days=1)\n",
    "    end_date_str = fmp_end_date.date().isoformat()\n",
    "\n",
    "    # Fetch account equity curve and benchmark data\n",
    "    portfolio_df = get_portfolio_history(manager, start_date, end_date)\n",
    "    benchmark_df = get_benchmark_data(start_date_str, end_date_str)\n",
    "\n",
    "    # Align metrics and plot (handles alignment and nulls internally)\n",
    "    display_metrics_table(portfolio_df, benchmark_df)\n",
    "    plot_performance(portfolio_df, benchmark_df)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Paper trading performance analysis failed:', e)\n",
    "    print('To enable this cell, configure Alpaca API in .env and ensure the account has order history.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancel All Open Alpaca Orders (Paper Trading)\n",
    "\n",
    "Use this cell to batch-cancel all pending (open) orders in your Alpaca paper trading account.\n",
    "This is useful when you want to revoke a rebalance before it executes, or clean up stale orders.\n",
    "\n",
    "**When to run:** Only as needed — e.g., if you submitted orders by mistake or want to cancel before market open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch cancel all open orders\n",
    "from src.trading.performance_analyzer import create_alpaca_account_from_env, AlpacaManager\n",
    "\n",
    "try:\n",
    "    account = create_alpaca_account_from_env()\n",
    "    manager = AlpacaManager([account])\n",
    "\n",
    "    # Query current open orders\n",
    "    open_orders = manager.get_orders(status='open', limit=200)\n",
    "    print('Open orders:', len(open_orders))\n",
    "\n",
    "    # Cancel all\n",
    "    cancelled = manager.cancel_all_orders()\n",
    "    print('Cancelled orders:', cancelled)\n",
    "\n",
    "except Exception as e:\n",
    "    print('Failed to cancel open orders:', e)\n",
    "    print('Please verify Alpaca API is configured and the account is set to Paper Trading.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
